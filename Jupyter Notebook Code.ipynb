{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7709d0c8",
   "metadata": {},
   "source": [
    "# Face Recognition based Lock using Convolutional Neural Network\n",
    "## Coded by Nishantkumar V Patel\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b11611c1",
   "metadata": {},
   "source": [
    "### Importing the relevant python libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8599da07",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-24 02:04:05.389127: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "from keras.layers import Conv2D, MaxPooling2D, Dense, Dropout, Flatten "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ee93b48",
   "metadata": {},
   "source": [
    "### Defining the function for making neural network algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7323ba48",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def build_Convolutional_Neural_Network():  # this is user defined function for building the machine learning algorithm.\n",
    "    CNN_model= Sequential()     #Sequential is used to initialize the neural network\n",
    "    \n",
    "    CNN_model.add(Conv2D(64, (3,3), input_shape = (128,128,3), activation = 'relu')) # input shape means, the dimensions (height, width, BGR) of images I want, then I will passing through this neural network for training-testing purposes.\n",
    "    CNN_model.add(MaxPooling2D(pool_size = (2,2))) # adding maxpooling2D layers.\n",
    "    \n",
    "    CNN_model.add(Conv2D(64, (3,3), activation = 'relu')) # (3,3) is filter size and 32 is filter numbers.\n",
    "    CNN_model.add(MaxPooling2D(pool_size = (2,2)))  \n",
    "    \n",
    "    CNN_model.add(Conv2D(128, (3,3), activation = 'relu')) # rectified linear unit is used. Also, 'elu' can be used.\n",
    "    CNN_model.add(MaxPooling2D(pool_size = (2,2))) \n",
    "    \n",
    "    CNN_model.add(Conv2D(128, (3,3), activation = 'relu'))\n",
    "    CNN_model.add(MaxPooling2D(pool_size = (2,2)))   # taking standard pool size. \n",
    "    \n",
    "    CNN_model.add(Flatten()) # converting the multidimensional tensors into 1D arrays with the help of Flatten layer.\n",
    "    \n",
    "    CNN_model.add(Dense(1024, activation = 'relu'))\n",
    "    CNN_model.add(Dense(128, activation = 'relu'))\n",
    "    CNN_model.add(Dense(64, activation = 'relu'))\n",
    "    CNN_model.add(Dense(2, activation = 'softmax')) # softmax is most suitable activation function used becuase it is used for Classification problem with probability distribution.\n",
    "                                                    #sigmoid can be sued as well instead of softmax as there is only 2 labels.\n",
    "    \n",
    "    \n",
    "    return CNN_model   # here return the thing I want from this user defined function which is CNN_model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b285e7f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-24 02:04:29.211671: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "CNN_model= build_Convolutional_Neural_Network()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d7273500",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d (Conv2D)             (None, 126, 126, 64)      1792      \n",
      "                                                                 \n",
      " max_pooling2d (MaxPooling2D  (None, 63, 63, 64)       0         \n",
      " )                                                               \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 61, 61, 64)        36928     \n",
      "                                                                 \n",
      " max_pooling2d_1 (MaxPooling  (None, 30, 30, 64)       0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " conv2d_2 (Conv2D)           (None, 28, 28, 128)       73856     \n",
      "                                                                 \n",
      " max_pooling2d_2 (MaxPooling  (None, 14, 14, 128)      0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " conv2d_3 (Conv2D)           (None, 12, 12, 128)       147584    \n",
      "                                                                 \n",
      " max_pooling2d_3 (MaxPooling  (None, 6, 6, 128)        0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 4608)              0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 1024)              4719616   \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 128)               131200    \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 64)                8256      \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 2)                 130       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 5,119,362\n",
      "Trainable params: 5,119,362\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "CNN_model.summary() # taking the overview of model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80c882f7",
   "metadata": {},
   "source": [
    "### Compiling the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "56847c7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "CNN_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy']) \n",
    "\n",
    "# binary crossentropy is used as there are only 2 labels either Nishant or Not Nishant for face lock system.\n",
    "# metrics is taking usually accuracy for better training results."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc7cdb4e",
   "metadata": {},
   "source": [
    "### Basic Image Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b98fd88a",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = ImageDataGenerator(rescale = 1./255, shear_range = 0.2, zoom_range = 0.2, horizontal_flip = True) \n",
    "\n",
    "test_dataset = ImageDataGenerator(rescale = 1./255)  # downsampling the images."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c5eeb4f",
   "metadata": {},
   "source": [
    "### Loading the Train and Test Images from my local directory folders."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b32722c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 500 images belonging to 2 classes.\n",
      "Found 100 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "train_data= train_dataset.flow_from_directory('/Users/jamesbond/Desktop/Face Lock for car security system/Train_Data', \n",
    "                                              target_size = (128,128), \n",
    "                                              batch_size = 20, # \n",
    "                                              class_mode = 'categorical')\n",
    "\n",
    "test_data= test_dataset.flow_from_directory('/Users/jamesbond/Desktop/Face Lock for car security system/Test_Data',\n",
    "                                             target_size = (128,128),  \n",
    "                                             batch_size = 20,  \n",
    "                                             class_mode = 'categorical')\n",
    "\n",
    "# total 500 images for training with Nishant & Not Nishant classes.\n",
    "#total 100 images for testing with Nishant & Not Nishant classes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1767fb28",
   "metadata": {},
   "source": [
    "### Feeding the image dataset into Neural network algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2f566761",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/p1/jsmxf7nd4hl8r1h69n1rhmkh0000gn/T/ipykernel_36244/3430596496.py:1: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n",
      "  model_training = CNN_model.fit_generator(train_data, #training the the train dataset at first.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "12/12 [==============================] - 13s 871ms/step - loss: 0.7002 - accuracy: 0.4750 - val_loss: 0.6916 - val_accuracy: 0.4333\n",
      "Epoch 2/15\n",
      "12/12 [==============================] - 20s 2s/step - loss: 0.6541 - accuracy: 0.5583 - val_loss: 0.5314 - val_accuracy: 0.4667\n",
      "Epoch 3/15\n",
      "12/12 [==============================] - 10s 790ms/step - loss: 0.7130 - accuracy: 0.6708 - val_loss: 0.6391 - val_accuracy: 0.6333\n",
      "Epoch 4/15\n",
      "12/12 [==============================] - 9s 719ms/step - loss: 0.5589 - accuracy: 0.7333 - val_loss: 0.3588 - val_accuracy: 0.9167\n",
      "Epoch 5/15\n",
      "12/12 [==============================] - 9s 767ms/step - loss: 0.3043 - accuracy: 0.8750 - val_loss: 0.2054 - val_accuracy: 0.9500\n",
      "Epoch 6/15\n",
      "12/12 [==============================] - 9s 784ms/step - loss: 0.0945 - accuracy: 0.9792 - val_loss: 0.0927 - val_accuracy: 0.9833\n",
      "Epoch 7/15\n",
      "12/12 [==============================] - 9s 777ms/step - loss: 0.1880 - accuracy: 0.9500 - val_loss: 0.0797 - val_accuracy: 0.9833\n",
      "Epoch 8/15\n",
      "12/12 [==============================] - 9s 738ms/step - loss: 0.1365 - accuracy: 0.9542 - val_loss: 0.1494 - val_accuracy: 0.9833\n",
      "Epoch 9/15\n",
      "12/12 [==============================] - 10s 827ms/step - loss: 0.1185 - accuracy: 0.9625 - val_loss: 0.1672 - val_accuracy: 0.9500\n",
      "Epoch 10/15\n",
      "12/12 [==============================] - 9s 747ms/step - loss: 0.0894 - accuracy: 0.9875 - val_loss: 0.1140 - val_accuracy: 0.9333\n",
      "Epoch 11/15\n",
      "12/12 [==============================] - 9s 732ms/step - loss: 0.0787 - accuracy: 0.9708 - val_loss: 0.0898 - val_accuracy: 1.0000\n",
      "Epoch 12/15\n",
      "12/12 [==============================] - 9s 705ms/step - loss: 0.0180 - accuracy: 0.9958 - val_loss: 0.0073 - val_accuracy: 1.0000\n",
      "Epoch 13/15\n",
      "12/12 [==============================] - 9s 754ms/step - loss: 0.0248 - accuracy: 0.9958 - val_loss: 0.0182 - val_accuracy: 0.9833\n",
      "Epoch 14/15\n",
      "12/12 [==============================] - 10s 833ms/step - loss: 0.0023 - accuracy: 1.0000 - val_loss: 0.0545 - val_accuracy: 0.9667\n",
      "Epoch 15/15\n",
      "12/12 [==============================] - 9s 705ms/step - loss: 0.2477 - accuracy: 0.9208 - val_loss: 0.3365 - val_accuracy: 0.9333\n"
     ]
    }
   ],
   "source": [
    "model_training = CNN_model.fit_generator(train_data, #training the the train dataset at first.\n",
    "                    steps_per_epoch = 12, # this means, number of batch passing per each epoch, as (250/ batch size)= 12.\n",
    "                    epochs = 15, #total epochs means for how many times I want to train or pass the data through out the whole neural network algorithm.\n",
    "                    validation_data = test_data, # here I used the test dataset as for validation.\n",
    "                    validation_steps = 3) #the number of batches per epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0db2b781",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['loss', 'accuracy', 'val_loss', 'val_accuracy'])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_training.history # it is a Dictionary contains the all model parameter records through out the whole training. \n",
    "model_training.history.keys() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2c594e02",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_training_Parameters= model_training.history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "43e6d28e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.4749999940395355,\n",
       " 0.5583333373069763,\n",
       " 0.6708333492279053,\n",
       " 0.7333333492279053,\n",
       " 0.875,\n",
       " 0.9791666865348816,\n",
       " 0.949999988079071,\n",
       " 0.9541666507720947,\n",
       " 0.9624999761581421,\n",
       " 0.987500011920929,\n",
       " 0.9708333611488342,\n",
       " 0.9958333373069763,\n",
       " 0.9958333373069763,\n",
       " 1.0,\n",
       " 0.9208333492279053]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_training_Parameters['accuracy']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "72e9063f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.4333333373069763,\n",
       " 0.46666666865348816,\n",
       " 0.6333333253860474,\n",
       " 0.9166666865348816,\n",
       " 0.949999988079071,\n",
       " 0.9833333492279053,\n",
       " 0.9833333492279053,\n",
       " 0.9833333492279053,\n",
       " 0.949999988079071,\n",
       " 0.9333333373069763,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.9833333492279053,\n",
       " 0.9666666388511658,\n",
       " 0.9333333373069763]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_training_Parameters['val_accuracy']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5012ca3",
   "metadata": {},
   "source": [
    "### Overviewing the parameters for model training "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "817e05a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "My model best Accuracy is: 1.0\n",
      "My model best Validation accuracy is: 1.0\n",
      "My model minimum Loss is: 0.002264350885525346\n",
      "My model minimum Validation loss is: 0.00729598430916667\n"
     ]
    }
   ],
   "source": [
    "print('My model best Accuracy is:', max(model_training_Parameters['accuracy']))\n",
    "print('My model best Validation accuracy is:', max(model_training_Parameters['val_accuracy']))\n",
    "print('My model minimum Loss is:', min(model_training_Parameters['loss']))\n",
    "print('My model minimum Validation loss is:',min(model_training_Parameters['val_loss']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51aefefd",
   "metadata": {},
   "source": [
    "### Saving the whole algorithm for future use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4014bcfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "CNN_model.save(\"CNN_Algorithm of Face Lock for Car.h5\") # saving the model as a new file to importing whenever its needed for further use."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "188c3093",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "## Now, integrating this Python based Code with Arduino Microcontroller using Pyserial..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cf3195b",
   "metadata": {},
   "source": [
    "### Importing the Python Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "78ba8520",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import load_model # for loading the previously saved model-\"CNN_Algorithm of Face Lock for Car.h5\"\n",
    "import serial  # to initialize the serial communication between Python and Arduino using Pyserial module.\n",
    "from matplotlib import pyplot as plt\n",
    "import cv2 as cv\n",
    "import dlib "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ed7634a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model= load_model(\"CNN_Algorithm of Face Lock for Car.h5\") # assigning that loaded model to the variable named model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e117cbc9",
   "metadata": {},
   "source": [
    "### Building the connection between Python and Arduino"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36d1464a",
   "metadata": {},
   "outputs": [],
   "source": [
    "arduino_signal= serial.Serial('/dev/cu.usbmodem14101', baudrate=9600) #calling up the arduino port name and standard baudrate of serial monitor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d645a2b",
   "metadata": {},
   "source": [
    "### Creating the web camera live detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7c9df49",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    " def live_detection():\n",
    "    web_cam = cv.VideoCapture(0) # 0 is for web camera.\n",
    "    face_detector = dlib.get_frontal_face_detector() #face detection function\n",
    "    trials=0\n",
    "    \n",
    "    while True:\n",
    "    \n",
    "        _, video_frame = web_cam.read()  \n",
    "        video_frame = cv.flip(video_frame, 1)\n",
    "        video_frame= cv.cvtColor(video_frame, cv.COLOR_BGR2RGB) \n",
    "    \n",
    "        faces = face_detector(video_frame)\n",
    "    \n",
    "        for face in faces:\n",
    "            x1 = int(face.left())     # mapping the exact face coordinates x and y.\n",
    "            y1 = int(face.top())\n",
    "            x2 = int(face.right())\n",
    "            y2 = int(face.bottom())\n",
    "            trials += 1\n",
    "            cropped_face = video_frame[y1:y2,x1:x2,:]  # cropping that mapped face image.\n",
    "          \n",
    "        cv.imshow(\"Face Recognizer\", cropped_face)       \n",
    "    \n",
    "        key = cv.waitKey(50)   # 50 miliseoconds are the frame refresh rate.\n",
    "        if (trials>1):     # if trials are more than two in that case it exited from the live detection function.\n",
    "            break   # stop the live camera \n",
    "            \n",
    "    web_cam.release() \n",
    "    \n",
    "    return cropped_face            \n",
    "\n",
    "driver_face= live_detection()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e188d6f",
   "metadata": {},
   "source": [
    "### Examining Nishant/ Not Nishant labels, and accordingly send the signals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dbb86df",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver_face= cv.resize(driver_face, (128,128))   # resizing the image as it will feed to neural network algorithm with (128,128) input shape.\n",
    "driver_face= np.reshape(driver_face, (1,128,128,3)) # reshaping the image with 4 dimesions. because Conv2D layers only work with 4 dimensions.\n",
    "model.predict_classes(driver_face)  # now feeding this live detection driver face into my saved algorithm for Nishant/ Not Nishant class prediction\n",
    "\n",
    "\n",
    "Final_Result= model.predict_classes(driver_face)[0] # to get an integer value from array\n",
    "\n",
    "\n",
    "if (model.predict(Final_Result)==1):  # if this is me then send the signal.\n",
    "    print(\"Access Accepted!\")\n",
    "    arduino_signal.write('1')     # sending the signal value '1' to serial monitor.\n",
    "    \n",
    "else:\n",
    "    print(\"Access Denied...\")     # this is not me. \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
